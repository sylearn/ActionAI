import asyncio
import json
import logging
import os
import tiktoken
import readline  # æ·»åŠ readlineå¯¼å…¥
from contextlib import AsyncExitStack
from typing import List

from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client
from openai import OpenAI
from rich.box import ROUNDED
from rich.console import Console, Group
from rich.live import Live
from rich.spinner import Spinner
from rich.markdown import Markdown
from rich.panel import Panel
from rich.table import Table
from rich.text import Text
from dataclasses import dataclass
import sys

def setup_base_paths():
    """
    è®¾ç½®åº”ç”¨ç¨‹åºçš„åŸºç¡€è·¯å¾„å¹¶æ·»åŠ åˆ°ç³»ç»Ÿè·¯å¾„
    
    Returns:
        str: åº”ç”¨ç¨‹åºçš„åŸºç¡€è·¯å¾„
    """
    # è·å–åº”ç”¨ç¨‹åºçš„åŸºç¡€è·¯å¾„
    if getattr(sys, 'frozen', False):
        # å¦‚æœæ˜¯æ‰“åŒ…åçš„åº”ç”¨
        base_path = os.path.dirname(sys.executable)  # è·å–å½“å‰åº”ç”¨çš„ç»å¯¹è·¯å¾„
    else:
        # å¦‚æœæ˜¯å¼€å‘ç¯å¢ƒ
        base_path = os.path.dirname(os.path.abspath(__file__))  # è·å–å½“å‰æ–‡ä»¶çš„ç»å¯¹è·¯å¾„

    # å°†åŸºç¡€è·¯å¾„æ·»åŠ åˆ°ç³»ç»Ÿè·¯å¾„
    # æ— è®ºç¨‹åºæ˜¯ä½œä¸ºè„šæœ¬è¿è¡Œè¿˜æ˜¯ä½œä¸ºæ‰“åŒ…åº”ç”¨è¿è¡Œï¼Œéƒ½èƒ½æ­£ç¡®å¯¼å…¥æ‰€éœ€çš„æ¨¡å—ã€‚
    sys.path.insert(0, base_path)
    if os.path.exists(os.path.join(base_path, '..')):
        sys.path.insert(0, os.path.abspath(os.path.join(base_path, '..')))
    
    return base_path

def preload_encodings():
    """
    é¢„åŠ è½½å¸¸ç”¨ç¼–ç å¹¶ä¸ºæ‰€æœ‰æ¨¡å‹æ³¨å†Œç›¸åŒçš„ç¼–ç 
    """
    try:
        # é¢„åŠ è½½å¸¸ç”¨ç¼–ç 
        logging.info("å¼€å§‹é¢„åŠ è½½tiktokenç¼–ç ...")
        # é¢„å…ˆå¯¼å…¥tiktokenæ‰©å±•æ¨¡å—ï¼Œè§£å†³æ‰“åŒ…åæ‰¾ä¸åˆ°ç¼–ç çš„é—®é¢˜
        import tiktoken_ext
        import tiktoken_ext.openai_public
        # å°è¯•åŠ è½½ç¼–ç 
        tiktoken.get_encoding("cl100k_base")
        logging.info("æˆåŠŸåŠ è½½cl100k_baseç¼–ç ")
        
        # ä¸ºæ‰€æœ‰æ¨¡å‹æ³¨å†Œç›¸åŒçš„ç¼–ç 
        # ä»ç¯å¢ƒå˜é‡è·å–æ¨¡å‹åˆ—è¡¨
        model_list_str = os.getenv("MODEL", "")
        logging.info(f"ç¯å¢ƒå˜é‡MODELå€¼: {model_list_str}")
        if model_list_str:
            # åˆ†å‰²æ¨¡å‹åˆ—è¡¨
            model_list = [model.strip() for model in model_list_str.split(";") if model.strip()]
            logging.info(f"è§£æå‡ºçš„æ¨¡å‹åˆ—è¡¨: {model_list}")
            # ä¸ºæ¯ä¸ªæ¨¡å‹æ³¨å†Œç›¸åŒçš„ç¼–ç 
            for model in model_list:
                if model:
                    try:
                        logging.info(f"æ­£åœ¨ä¸ºæ¨¡å‹ {model} æ³¨å†Œcl100k_baseç¼–ç ")
                        tiktoken.model.MODEL_TO_ENCODING[model] = "cl100k_base"
                        logging.info(f"æ¨¡å‹ {model} æ³¨å†ŒæˆåŠŸ")
                    except Exception as inner_e:
                        logging.error(f"ä¸ºæ¨¡å‹ {model} æ³¨å†Œç¼–ç æ—¶å‡ºé”™: {inner_e}")  
                        import traceback
                        logging.error(traceback.format_exc())
    except Exception as e:
        logging.error(f"é¢„åŠ è½½ tiktoken ç¼–ç å¤±è´¥: {e}")
        import traceback
        logging.error(traceback.format_exc())

# åˆå§‹åŒ–åŸºç¡€è·¯å¾„
base_path = setup_base_paths()

from utils.utils import parse_mcp_servers, get_all_server_names, get_server_command, get_server_args, get_server_env
from utils.utils import get_token_count
from utils.utils import load_env_files

# åŠ è½½ç¯å¢ƒå˜é‡
load_env_files(seconds=1)

# é¢„åŠ è½½ç¼–ç  - ç§»åˆ°åŠ è½½ç¯å¢ƒå˜é‡ä¹‹å
preload_encodings()

def setup_logging():
    """
    é…ç½®æ—¥å¿—ç³»ç»Ÿ
    
    æ ¹æ®ç¯å¢ƒå˜é‡DEBUGè®¾ç½®æ—¥å¿—çº§åˆ«å’Œè¾“å‡ºæ–¹å¼
    """
    # é…ç½®æ—¥å¿—çº§åˆ«
    # é…ç½®æ—¥å¿—è®°å½•å™¨
    logger = logging.getLogger()
    logger.setLevel(logging.DEBUG if os.getenv("DEBUG", "False").lower() == "true" else logging.ERROR)

    # é…ç½®ç»Ÿä¸€çš„æ—¥å¿—æ ¼å¼
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')

    # é…ç½®æ§åˆ¶å°å¤„ç†å™¨
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.ERROR)
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)

    # ä»…åœ¨debugæ¨¡å¼ä¸‹æ·»åŠ æ–‡ä»¶å¤„ç†å™¨
    if os.getenv("DEBUG", "False").lower() == "true":
        # é…ç½®æ–‡ä»¶å¤„ç†å™¨
        file_handler = logging.FileHandler('client_dev_debug.log', encoding='utf-8')
        file_handler.setLevel(logging.DEBUG)
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)
    
    return logger

# è®¾ç½®æ—¥å¿—
logger = setup_logging()

@dataclass
class LLMConfig:
    """LLMé…ç½®ç±»"""
    # æµå¼è¾“å‡ºè®¾ç½®
    stream: bool = True if os.getenv("STREAM", "True").lower() == "true" else False
    # æ¨¡å‹å‚æ•°è®¾ç½®
    model: str = os.getenv("MODEL", "")
    max_tokens: int = int(os.getenv("MAX_TOKENS", 4096))
    temperature: float = float(os.getenv("TEMPERATURE", 0.7))
    # è·å–ç³»ç»Ÿæç¤ºæ–‡ä»¶è·¯å¾„ï¼Œä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡ï¼Œå¦åˆ™ä½¿ç”¨åŸºç¡€è·¯å¾„ä¸‹çš„é»˜è®¤æ–‡ä»¶
    system_prompt: str = os.getenv("SYSTEM_PROMPT", os.path.join(base_path, "prompt/system.md"))
    # è°ƒè¯•å’ŒåŠŸèƒ½è®¾ç½®
    debug: bool = True if os.getenv("DEBUG", "False").lower() == "true" else False
    api_key: str = os.getenv("OPENAI_API_KEY", "")
    base_url: str = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
    is_function_calling: bool = True if os.getenv("IS_FUNCTION_CALLING", "True").lower() == "true" else False
    is_human_control: bool = True if os.getenv("IS_HUMAN_CONTROL", "True").lower() == "true" else False
    # è¶…æ—¶å’Œé™åˆ¶è®¾ç½®
    timeout: int = int(os.getenv("TIMEOUT", 300))
    max_messages: int = int(os.getenv("MAX_MESSAGES", 100))
    max_messages_tokens: int = int(os.getenv("MAX_MESSAGES_TOKENS", 60000))
    # è·¯å¾„è®¾ç½®
    messages_path: str = os.getenv("MESSAGES_PATH", os.path.join(base_path, "messages"))
    mcp_server_config: str = os.getenv("MCP_SERVER_CONFIG_PATH", os.path.join(base_path, "mcp_server_config.json"))
    python_path: str = os.getenv("PYTHON_PATH", "python")
    node_path: str = os.getenv("NODE_PATH", "node")
    npx_path: str = os.getenv("NPX_PATH", "npx")

def load_config_from_env(base_path: str) -> LLMConfig:
    """
    ä»ç¯å¢ƒå˜é‡åŠ è½½é…ç½®
    
    Args:
        base_path: åº”ç”¨ç¨‹åºåŸºç¡€è·¯å¾„
        
    Returns:
        LLMConfig: é…ç½®å¯¹è±¡
    """
    return LLMConfig(
        stream=True if os.getenv("STREAM", "True").lower() == "true" else False,
        model=os.getenv("MODEL", ""),
        max_tokens=int(os.getenv("MAX_TOKENS", 4096)),
        temperature=float(os.getenv("TEMPERATURE", 0.7)),
        system_prompt=os.getenv("SYSTEM_PROMPT", os.path.join(base_path, "prompt/system.md")),
        debug=True if os.getenv("DEBUG", "False").lower() == "true" else False,
        api_key=os.getenv("OPENAI_API_KEY", ""),
        base_url=os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1"),
        is_function_calling=True if os.getenv("IS_FUNCTION_CALLING", "True").lower() == "true" else False,
        is_human_control=True if os.getenv("IS_HUMAN_CONTROL", "True").lower() == "true" else False,
        timeout=int(os.getenv("TIMEOUT", 300)),
        max_messages=int(os.getenv("MAX_MESSAGES", 100)),
        max_messages_tokens=int(os.getenv("MAX_MESSAGES_TOKENS", 60000)),
        messages_path=os.getenv("MESSAGES_PATH", os.path.join(base_path, "messages")),
        mcp_server_config=os.getenv("MCP_SERVER_CONFIG_PATH", os.path.join(base_path, "mcp_server_config.json")),
        python_path=os.getenv("PYTHON_PATH", "python"),
        node_path=os.getenv("NODE_PATH", "node"),
        npx_path=os.getenv("NPX_PATH", "npx")
    )

class LLM_Client:
    def __init__(self, config: LLMConfig = None):
        """
        åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
        Args:
            config: LLMé…ç½®å¯¹è±¡ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
        """
        # ä½¿ç”¨æä¾›çš„é…ç½®æˆ–åˆ›å»ºé»˜è®¤é…ç½®
        self.config = config or load_config_from_env(base_path)
        # MCPç›¸å…³å±æ€§
        self.server_names: List[str] = []
        self.session_list: List[ClientSession] = []
        self.available_tools: List = []
        self.exit_stack = AsyncExitStack()
        
        # OpenAIç›¸å…³å±æ€§
        self.openai = OpenAI(
            api_key=self.config.api_key,
            base_url=self.config.base_url
        )
        self.total_input_tokens = 0
        self.total_output_tokens = 0
        self.current_input_tokens = 0
        self.thinking_tokens = 0
        self.round_count = 0 # å½“å‰è½®æ¬¡
        self.tool_tokens = []
        # æ¶ˆæ¯ç›¸å…³å±æ€§
        self.messages: List = []
        self._console = Console()
        
        # è¾“å…¥æç¤ºç¬¦
        self.input_prompt = "\n[bold yellow]User:[/bold yellow] (æŒ‰Enteræ¢è¡Œï¼Œè¾“å…¥'\\q'ç»“æŸï¼Œ'\\c'æ¸…é™¤)\n"

    def _models_ifc(self):
        """åˆ¤æ–­æ¨¡å‹æ˜¯å¦æ”¯æŒå·¥å…·è°ƒç”¨"""
        if self.model in ['deepseek-r1']:
            self.config.is_function_calling = False
        else:
            self.config.is_function_calling = True
            
    def choose_model(self):
        """é€‰æ‹©è¦ä½¿ç”¨çš„AIæ¨¡å‹"""
        
        # ç›´æ¥è¯»å–ç¯å¢ƒå˜é‡
        try:
            # ä»ç¯å¢ƒå˜é‡ä¸­è·å–æ¨¡å‹åˆ—è¡¨å¹¶å¤„ç†
            model_list = [model.strip() for model in self.config.model.split(";") if model.strip() and len(model.strip()) > 0]
        except:
            return ValueError("ç¯å¢ƒå˜é‡MODELæ ¼å¼é”™è¯¯")
        # æ¸²æŸ“çŠ¶æ€
        def render_status(message="", style="cyan"):
            # åˆ›å»ºä¸€ä¸ªæ›´ç¾è§‚çš„è¡¨æ ¼
            model_table = Table(
                show_header=True,
                header_style="bold cyan",
                box=ROUNDED,
                expand=True,
                padding=(0, 1)  # æ·»åŠ ä¸€äº›å†…è¾¹è·
            )
            
            # æ·»åŠ åˆ—ï¼Œè®¾ç½®åˆé€‚çš„å®½åº¦å’Œå¯¹é½æ–¹å¼
            model_table.add_column("åºå·", justify="center", width=6)
            model_table.add_column("æ¨¡å‹åç§°", justify="left")
            
            # æ·»åŠ è¡Œï¼Œä½¿ç”¨æ›´å¥½çš„æ ¼å¼
            for i, model in enumerate(model_list, 1):
                model_table.add_row(
                    f"{i}",
                    model,
                    style="white"
                )

            # åˆ›å»ºé¢æ¿
            status_panel = Panel(
                Group(
                    model_table,
                    Text("\n" + message, style=style) if message else "",
                ),
                title=f"[bold]ğŸ¤– æ¨¡å‹é€‰æ‹© [/bold]",
                border_style=style,
                padding=(1, 2)  # æ·»åŠ é¢æ¿å†…è¾¹è·
            )
            return status_panel
        console=self._console
        with Live(render_status(), console=console, auto_refresh=False) as live:
            render_status_message=""
            while True:
                live.stop()  # æš‚åœLiveæ˜¾ç¤ºï¼Œå…‰æ ‡åœ¨Liveä¸‹æ–¹
                try:
                    #é˜²æ­¢ä¿¡æ¯è¾“å…¥åˆ°Consoleä¸Šæ–¹
                    choice_input = console.input("ğŸ”¢ è¯·è¾“å…¥æ‚¨æƒ³ä½¿ç”¨çš„æ¨¡å‹ç¼–å·: ")
                    live.start()  # æ¢å¤Liveæ˜¾ç¤ºï¼Œå…‰æ ‡åœ¨Liveä¸Šæ–¹
                    #ç›´æ¥å›è½¦åˆ™é€‰æ‹©é»˜è®¤ç¬¬ä¸€ä¸ªæ¨¡å‹
                    if not choice_input.strip():
                        choice = 1
                    else:
                        choice = int(choice_input)
                    #ç›´æ¥å›è½¦åˆ™é€‰æ‹©é»˜è®¤ç¬¬ä¸€ä¸ªæ¨¡å‹
                    if not choice:
                        choice = 1
                    
                    if 1 <= choice <= len(model_list):
                        self.model = model_list[choice - 1]
                        render_status_message = f"âœ… æ‚¨å·²é€‰æ‹©æ¨¡å‹: {self.model}"
                        live.update(render_status(render_status_message, "green"), refresh=True)
                        console.clear()  #æ¸…ç©ºå±å¹•ã€‚ä½†ä¿ç•™æœ€åä¸€æ¬¡çš„è¾“å‡ºåœ¨å±å¹•ä¸Š
                        #åˆ¤æ–­æ¨¡å‹æ˜¯å¦æ”¯æŒå·¥å…·è°ƒç”¨
                        self._models_ifc()
                        #è¿”å›é€‰æ‹©çš„æ¨¡å‹
                        return None
                    
                    else:
                        render_status_message = f"âŒ è¯·è¾“å…¥1åˆ°{len(model_list)}ä¹‹é—´çš„æ•°å­—"
                        #æ›´æ–°çŠ¶æ€æ¶ˆæ¯
                        #console.clear()
                        live.update(render_status(render_status_message, "yellow"), refresh=True)
                        console.clear()
                except ValueError:
                    live.start()  # ç¡®ä¿åœ¨æ˜¾ç¤ºé”™è¯¯æ—¶Liveæ˜¯å¼€å¯çš„
                    render_status_message = "â— è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—"
                    live.update(render_status(render_status_message, "red"), refresh=True)
                    console.clear()
                    

    def get_response(self) -> tuple[list, list]:
        """ä»OpenAIè·å–å“åº”å¹¶å¤„ç†å·¥å…·è°ƒç”¨

        Args:
            messages: å¯¹è¯å†å²æ¶ˆæ¯åˆ—è¡¨

        Returns:
            tuple[list, list]: (å·¥å…·è°ƒç”¨åˆ—è¡¨, å“åº”æ¶ˆæ¯åˆ—è¡¨)
        """
        [logging.debug(message) for message in self.messages]
        if self.config.is_function_calling:
            # è®¡ç®—tool tokens
            encoding = tiktoken.encoding_for_model(self.model)
            name = [json.dumps(tool["function"]["name"]) for available_tool in self.available_tools for tool in available_tool]
            parameters = [json.dumps(tool["function"]["parameters"]) for available_tool in self.available_tools for tool in available_tool]
            description = [json.dumps(tool["function"]["description"]) for available_tool in self.available_tools for tool in available_tool]

            # æ¯ä¸ªå­—ç¬¦ä¸²çš„tokenæ•°ä¹‹å’Œä¸ºtool_tokens
            self.tool_tokens.append(sum([len(encoding.encode(n)) for n in name]) + sum([len(encoding.encode(p)) for p in parameters]))
            completion = self.openai.chat.completions.create(
                model=self.model,
                max_tokens=self.config.max_tokens,
                messages=self.messages,
                tools=[tool for available_tool in self.available_tools for tool in available_tool],
                stream=self.config.stream,
                temperature=self.config.temperature,
                timeout=self.config.timeout
            )
        else:
            self.tool_tokens.append(0)
            completion = self.openai.chat.completions.create(
                model=self.model,
                max_tokens=self.config.max_tokens,
                messages=self.messages,
                stream=self.config.stream,
                temperature=self.config.temperature,
                timeout=self.config.timeout
            )
        
        # å¤„ç†æµå¼å“åº”
        if self.config.stream:
            return self._handle_stream_response(completion)
        # å¤„ç†æ™®é€šå“åº”
        else:
            return self._handle_normal_response(completion)
    #å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œè´Ÿè´£å¤„ç†ç»ˆç«¯çš„è¾“å‡ºï¼ŒåŒ…å«æ€è€ƒå’Œå“åº”ï¼Œå¹¶ä¸”åˆ·æ–°ç»ˆç«¯
    def _print_thinking_and_response(self, chunk, live: Live,thinking_content: str="", full_content: str="",spinner: Spinner=None) -> tuple[str,str]:
        """æ‰“å°æ€è€ƒå’Œå“åº”ï¼Œå¹¶åˆ·æ–°ç»ˆç«¯
        Args:
            chunk: å“åº”å—/completion
            live: ç»ˆç«¯å¯¹è±¡
        """
        output_content = ""
        if self.config.stream:
            # æ›´æ–°å†…å®¹
            delta = chunk.choices[0].delta
            if hasattr(delta, 'reasoning_content') and delta.reasoning_content:
                thinking_content += delta.reasoning_content
            if hasattr(delta, 'content') and delta.content:
                full_content += delta.content
            
            title_content = f"{self.model}"
            
            # åˆ†åˆ«å¤„ç†æ€è€ƒå†…å®¹å’Œå“åº”å†…å®¹
            if thinking_content:
                if spinner:
                    spinner.text = "ğŸ¤” æ€è€ƒä¸­... | Thinking..."
                output_content += f"# Thinking\n\n```markdown\n{thinking_content}\n```\n\n"
            
            if full_content:
                if spinner:
                    spinner.text = "ğŸ’¬ æ­£åœ¨è¾“å‡º... | Outputting..."
                output_content += f"# Response\n\n{full_content}"
            
            # åªæœ‰åœ¨æœ‰å†…å®¹æ—¶æ‰æ›´æ–°æ˜¾ç¤º
            if output_content:
                if spinner:
                    panel = Panel(
                        Group(spinner, Markdown(output_content)),
                        title=title_content
                    )
                else:
                    panel = Panel(
                        Markdown(output_content),
                        title=title_content
                    )
                live.update(panel, refresh=True)
            return thinking_content, full_content

        else:
            #è·å–æ€è€ƒå’Œå“åº”å†…å®¹
            thinking_content =chunk.choices[0].message.reasoning_content if hasattr(chunk.choices[0].message, 'reasoning_content') and chunk.choices[0].message.reasoning_content else ""
            full_content = chunk.choices[0].message.content if hasattr(chunk.choices[0].message, 'content') and chunk.choices[0].message.content else ""
            #æ‰“å°tokenæ•°é‡
            title_content = f"{self.model}"
            if thinking_content != "":
                output_content +=  f"# Thinking...\n\n```markdown\n\n"+thinking_content+ "\n```\n"
            if full_content != "":
                output_content += f"# Response\n\n"+full_content
            # ä½¿ç”¨Panelæ˜¾ç¤ºå†…å®¹ï¼Œæ·»åŠ tokenä¿¡æ¯ä½œä¸ºsubtitle
            live.update(Panel(
                Markdown(output_content),
                #subtitle=f"[dim]{token_info}[/dim]",
                title=title_content
            ), refresh=True)

        return thinking_content, full_content

    def _handle_stream_response(self, completion) -> tuple[list, list]:
        """å¤„ç†æµå¼å“åº”

        Args:
            completion: OpenAIçš„æµå¼å“åº”å¯¹è±¡

        Returns:
            tuple[list, list]: (å·¥å…·è°ƒç”¨åˆ—è¡¨, å“åº”æ¶ˆæ¯åˆ—è¡¨)
        """
        tool_functions_index = {}
        thinking_content=""
        full_content =  ""
        current_index=0
        #éå†å“åº”å—
        # åˆ›å»ºspinner
        spinner = Spinner('dots', text='ğŸ’¬ æ­£åœ¨è¾“å‡ºä¸­ | Outputting...')
        with Live(console=self._console, auto_refresh=False,vertical_overflow="ellipsis") as live:
            for chunk in completion:
                logging.debug(chunk)
                #å¤„ç†å·¥å…·è°ƒç”¨
                if tool_calls := chunk.choices[0].delta.tool_calls:
                    #æµå¼å“åº”tool_callsåˆ—è¡¨åªåŒ…å«ä¸€ä¸ªå¯¹è±¡ï¼Œè·å–å·¥å…·åç§°å’Œå‚æ•°
                    full_content,current_index = self._process_stream_tool_call(chunk, tool_calls[0], tool_functions_index, full_content, current_index, live,spinner)
                    
                #æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œç›´æ¥æ‰“å°å†…å®¹
                elif chunk.choices[0].delta.content or (hasattr(chunk.choices[0].delta, 'reasoning_content') and chunk.choices[0].delta.reasoning_content):
                    thinking_content,full_content = self._process_stream_content(chunk,thinking_content,full_content, live,spinner)
            #æœ€åæ›´æ–°spinner
            spinner.text = "âœ¨ è¾“å‡ºå®Œæˆ | Output completed âœ…"
            tool_functions = [v for k, v in tool_functions_index.items()]
            #å¦‚æœæœ‰æ€ç»´ï¼Œåˆ™è®¡ç®—æ€ç»´token
            if thinking_content:
                encoding = tiktoken.encoding_for_model(self.model)
                self.thinking_tokens += len(encoding.encode(thinking_content))
        return tool_functions, self._create_response(full_content, tool_functions)

    def _process_stream_tool_call(self, chunk, tool_call, tool_functions_index: dict, full_content: str, current_index: int, live: Live,spinner: Spinner) -> str:
        """å¤„ç†OpenAIæµå¼å“åº”ä¸­çš„å·¥å…·è°ƒç”¨ã€‚OpenAIçš„å·¥å…·è°ƒç”¨å“åº”æ˜¯åˆ†å—å‘é€çš„ï¼Œä¸»è¦æœ‰ä¸¤ç§ç±»å‹ï¼š

        1. æ–°å·¥å…·è°ƒç”¨çš„å¼€å§‹ï¼š
           - åŒ…å«å®Œæ•´çš„è°ƒç”¨ä¿¡æ¯ï¼ˆid, name, typeç­‰ï¼‰
           - idä¸ä¸ºç©ºï¼Œè¡¨ç¤ºè¿™æ˜¯ä¸€ä¸ªæ–°çš„å·¥å…·è°ƒç”¨
           ç¤ºä¾‹ï¼š
           {
               "index": 0,
               "id": "call_xxx",
               "function": {"arguments": "", "name": "get_weather"},
               "type": "function"
           }

        2. å·¥å…·å‚æ•°çš„æŒç»­ä¼ è¾“ï¼š
           - idä¸ºç©ºï¼Œè¡¨ç¤ºè¿™æ˜¯å½“å‰å·¥å…·è°ƒç”¨çš„å‚æ•°å†…å®¹
           - å‚æ•°å†…å®¹ä¼šåˆ†å¤šæ¬¡å‘é€ï¼Œéœ€è¦æ‹¼æ¥
           ç¤ºä¾‹ï¼š
           {"index": 0, "id": null, "function": {"arguments": "{\\"city\\": \\"", "name": null}}
           {"index": 0, "id": null, "function": {"arguments": "Beijing\\"}", "name": null}}

        å®ç°é€»è¾‘ï¼š
        1. ä½¿ç”¨current_indexè®°å½•å½“å‰æ­£åœ¨å¤„ç†çš„å·¥å…·è°ƒç”¨
        2. å½“æ”¶åˆ°idä¸ä¸ºç©ºçš„chunkæ—¶ï¼Œåˆ›å»ºæ–°çš„å·¥å…·è°ƒç”¨è®°å½•
        3. å½“æ”¶åˆ°idä¸ºç©ºçš„chunkæ—¶ï¼Œå°†å†…å®¹è¿½åŠ åˆ°current_indexå¯¹åº”çš„å·¥å…·è°ƒç”¨ä¸­
        4. ä¸ç”¨indexæ¥åŒºåˆ†ä¸åŒçš„å·¥å…·è°ƒç”¨æ˜¯å› ä¸ºclaudeç­‰å…¶ä»–æ¨¡å‹çš„indexå§‹ç»ˆä¸ºç©º
        Args:
            chunk: å“åº”å—
            tool_call: å·¥å…·è°ƒç”¨å¯¹è±¡
            tool_functions_index: å·¥å…·è°ƒç”¨ç´¢å¼•å­—å…¸
            content: å½“å‰ç´¯ç§¯çš„å†…å®¹
            current_index: å½“å‰ç´¢å¼•

        Returns:
            str: æ›´æ–°åçš„å†…å®¹
        """
        #å¦‚æœè°ƒç”¨å·¥å…·æ—¶å†…å®¹ä¸ä¸ºç©ºï¼Œæ‰“å°å¹¶ç´¯åŠ 
        if chunk.choices[0].delta.content:
            full_content += chunk.choices[0].delta.content
            
            output_content=f"# Response\n\n"+full_content
            if spinner:
                spinner.text = "ğŸ’¬ æ­£åœ¨è¾“å‡º... | Outputting..."
                display_content = Panel(
                    Group(spinner, Markdown(output_content)),
                    #subtitle=f"[dim]{token_info}[/dim]"
                )
                live.update(display_content, refresh=True)
            else:
                live.update(Panel(
                    Markdown(output_content),
                    #subtitle=f"[dim]{token_info}[/dim]"
                ), refresh=True)
            logging.debug("æ—¢æœ‰æœ‰tool_callså­—æ®µï¼Œåˆæœ‰contentå­—æ®µã€‚")
        else:
            if spinner:
                spinner.text = "ğŸ”§ æ­£åœ¨å‡†å¤‡å·¥å…·è°ƒç”¨ä¸­... | Preparing tool calls..."
                output_content=f"# Response\n\n"+full_content
                display_content = Panel(
                    Group(spinner, Markdown(output_content)),
                    #subtitle=f"[dim]{token_info}[/dim]"
                )
                live.update(display_content, refresh=True)
            else:
                live.update(Panel(
                    Markdown(full_content+"\n\n # æ­£åœ¨å‡†å¤‡å·¥å…·è°ƒç”¨ï¼Œè¯·ç¨å..."),
                    #subtitle=f"[dim]{token_info}[/dim]"
                ), refresh=True)
            logging.debug("åªæœ‰tool_callså­—æ®µã€‚")
           
    
        #è·å–å·¥å…·åç§°å’Œå‚æ•°
        tool_name = tool_call.function.name if tool_call.function.name else ""
        tool_args = tool_call.function.arguments if tool_call.function.arguments else ""
        #å¦‚æœå·¥å…·è°ƒç”¨IDä¸ºç©ºï¼Œè¯´æ˜æ˜¯æ—§çš„è°ƒç”¨
        if not tool_call.id:
            tool_functions_index[current_index]["function"]["name"] += tool_name
            tool_functions_index[current_index]["function"]["arguments"] += tool_args
            
        #å¦‚æœå·¥å…·è°ƒç”¨IDä¸ä¸ºç©ºï¼Œè¯´æ˜æ˜¯æ–°çš„è°ƒç”¨
        else:
            #æ›´æ–°ç´¢å¼•
            current_index += 1
            tool_functions_index[current_index] = {
                "id": tool_call.id or "",
                "type": tool_call.type or "",
                "function": {"name": tool_name, 
                             "arguments": tool_args}
            }
        return full_content,current_index
    

    def _process_stream_content(self, chunk, thinking_content: str, full_content: str, live: Live,spinner: Spinner) -> str:
        """å¤„ç†æµå¼å“åº”ä¸­çš„æ™®é€šå†…å®¹
            å“åº”å—çš„æ ¼å¼å¦‚ä¸‹
            ChatCompletionChunk(id='chatcmpl-BiDxhaqknhcZCUs1A22bV0U9972719HG', 
                                choices=[Choice(delta=ChoiceDelta(content='***', function_call=None, refusal=None, role='assistant', tool_calls=None), 
                                         finish_reason=None, index=0, logprobs=None)], 
                                created=1740117256, model='gpt-4o-2024-08-06', 
                                object='chat.completion.chunk', 
                                service_tier=None, 
                                system_fingerprint='', 
                                usage=None)
        Args:
            chunk: å“åº”å—
            content: å½“å‰ç´¯ç§¯çš„å†…å®¹

        Returns:
            str: æ›´æ–°åçš„å†…å®¹
        """
        logging.debug("æ²¡æœ‰tool_callså­—æ®µï¼Œæœ‰contentå­—æ®µ:")
        logging.debug(chunk)
        # æ›´æ–°å†…å®¹
        thinking_content, full_content = self._print_thinking_and_response(chunk, live, thinking_content, full_content,spinner)
        
        return thinking_content,full_content

    def _handle_normal_response(self, completion) -> tuple[list, list]:
        """å¤„ç†æ™®é€šï¼ˆéæµå¼ï¼‰å“åº”
        Args:
            completion: OpenAIçš„å“åº”å¯¹è±¡

        Returns:
            tuple[list, list]: (å·¥å…·è°ƒç”¨åˆ—è¡¨, å“åº”æ¶ˆæ¯åˆ—è¡¨)
        """
        full_content = ""
        tool_functions = []
        logging.debug(completion)
        # å¦‚æœæœ‰å·¥å…·è°ƒç”¨
        if tool_calls := completion.choices[0].message.tool_calls:
            #å¦‚æœè°ƒç”¨å·¥å…·æ—¶å€™è¿˜æœ‰æ–‡æœ¬å†…å®¹ï¼Œåˆ™æ‰“å°
            if completion.choices[0].message.content:
                with Live(console=self._console, auto_refresh=False) as live:
                    thinking_content, full_content = self._print_thinking_and_response(completion, live)
            #éå†å·¥å…·è°ƒç”¨
            for tool_call in tool_calls:
                tool_functions.append({
                    #"index": tool_call.index,
                    "id": tool_call.id,
                    "type": tool_call.type,
                    "function": {
                        "name": tool_call.function.name,
                        "arguments": tool_call.function.arguments
                    }
                })
                
        # æ²¡æœ‰å·¥å…·è°ƒç”¨
        else:
            with Live(console=self._console, auto_refresh=False) as live:
                thinking_content, full_content = self._print_thinking_and_response(completion, live)
        
        return tool_functions, self._create_response(full_content, tool_functions)

    def _create_response(self, full_content: str, tool_functions: list) -> list:
        """åˆ›å»ºå“åº”æ¶ˆæ¯åˆ—è¡¨

        Args:
            content: å“åº”å†…å®¹
            tool_functions: å·¥å…·è°ƒç”¨åˆ—è¡¨

        Returns:
            list: å“åº”æ¶ˆæ¯åˆ—è¡¨
        """
        if not tool_functions:
            return [{
                "role": "assistant",
                "content": full_content,
            }]
        
        response = []
        for tool_function in tool_functions:
            response.append({
                "role": "assistant",
                "content": full_content or f"Tool Function Called: {tool_function['function']['name']}\nArguments: {tool_function['function']['arguments'][:300]}{'...' if len(tool_function['function']['arguments']) > 200 else ''}", # Claude requires non-empty content
                "function_call": {
                    "name": tool_function["function"]["name"],
                    "arguments": tool_function["function"]["arguments"]
                }
            })
        return response

    async def get_tools(self):
        for session in self.session_list:
            # åˆ—å‡ºå¯ç”¨å·¥å…·
            available_tools = [] 
            response = await session.list_tools()
            for tool in response.tools:
                available_tools.append({
                    "type": "function",
                    "function": {
                        "name": tool.name,
                        "description": tool.description,
                        "parameters": tool.inputSchema
                    }
                })
            self.available_tools.append(available_tools)
        
        if not self.available_tools:
            self._console.print(Panel("[yellow]è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å¯ç”¨å·¥å…·ã€‚[/yellow]"))
        else:
            total_tools = sum(len(tools) for tools in self.available_tools)
            self._console.print(Panel(f"[green]æˆåŠŸè·å–åˆ° {len(self.session_list)} ä¸ªæœåŠ¡å™¨ï¼Œæ€»è®¡ {total_tools} ä¸ªå·¥å…·ã€‚[/green]"))
    async def connect_to_server(self):
        """è¿æ¥åˆ° MCP æœåŠ¡å™¨
    
        å‚æ•°:
            server_script_path: æœåŠ¡å™¨è„šæœ¬è·¯å¾„ (.py æˆ– .js)
        """
        if self.config.mcp_server_config:
            try:
                # è§£æé…ç½®æ–‡ä»¶
                servers_info = parse_mcp_servers(self.config.mcp_server_config)
                # è·å–æ‰€æœ‰æœåŠ¡å™¨åç§°
                server_names = get_all_server_names(servers_info)
                #print(f"æœåŠ¡å™¨åˆ—è¡¨: {server_names}")
                
                # éå†æ¯ä¸ªæœåŠ¡å™¨ï¼Œä¸ºæ¯ä¸ªæœåŠ¡å™¨åˆ›å»ºå•ç‹¬çš„è¿æ¥
                for server_name in server_names:
                    command = get_server_command(servers_info, server_name)
                    args = get_server_args(servers_info, server_name)
                    env = get_server_env(servers_info, server_name)  
                    
                    # æ£€æŸ¥å‘½ä»¤æ˜¯å¦ä¸º npx
                    if command.lower() == "npx":
                        command = self.config.npx_path
                        # å¦‚æœæ˜¯ npx å‘½ä»¤ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦è¿æ¥åˆ°ç½‘ç»œ
                        if any("@modelcontextprotocol" in arg for arg in args):
                            # è¿™æ˜¯ä¸€ä¸ªéœ€è¦ä»ç½‘ç»œä¸‹è½½çš„åŒ…ï¼Œå¯èƒ½ä¼šé‡åˆ°ç½‘ç»œé—®é¢˜
                            self._console.print(Panel(
                                Markdown(f"ğŸŒ **æ­£åœ¨ä»ç½‘ç»œä¸‹è½½å¹¶å®‰è£…æœåŠ¡: {server_name}**\n\nè¯·è€å¿ƒç­‰å¾…ï¼Œè¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´..."),
                                title="[bold blue]è¿œç¨‹å®‰è£…è¿›è¡Œä¸­[/bold blue]",
                                border_style="blue",
                                expand=False
                            ))
                            self._console.print(f"[dim]å‘½ä»¤: {command} {' '.join(args)}[/dim]")
                    elif command.lower() == "python":
                        command = self.config.python_path
                        self._console.print(f"[dim]å‘½ä»¤: {command} {' '.join(args)}[/dim]")
                    elif command.lower() == "node":
                        command = self.config.node_path
                        self._console.print(f"[dim]å‘½ä»¤: {command} {' '.join(args)}[/dim]")
                    else:
                        # å¯¹äºå…¶ä»–å‘½ä»¤ï¼Œä¹Ÿä½¿ç”¨åŸå§‹å­—ç¬¦ä¸²
                        command = rf"{command}"
                        self._console.print(f"[dim]å‘½ä»¤: {command} {' '.join(args)}[/dim]")

                    # åˆ›å»º StdioServerParameters å¯¹è±¡
                    server_params = StdioServerParameters(
                        command=command,
                        args=args,
                        env={
                            **os.environ.copy(),  # å¤åˆ¶å½“å‰ç¯å¢ƒå˜é‡
                            # ç¡®ä¿å…³é”®è·¯å¾„é…ç½®è¢«æ­£ç¡®ä¼ é€’
                            "PYTHON_PATH": self.config.python_path,
                            "NODE_PATH": self.config.node_path,
                            "NPX_PATH": self.config.npx_path
                        }
                    )
                
                    try:
                        # ä½¿ç”¨ stdio_client åˆ›å»ºä¸æœåŠ¡å™¨çš„ stdio ä¼ è¾“
                        studiocilent=stdio_client(server_params)
                        stdio_transport = await self.exit_stack.enter_async_context(studiocilent)
                        # è§£åŒ… stdio_transportï¼Œè·å–è¯»å–å’Œå†™å…¥å¥æŸ„
                        stdio, write = stdio_transport
                        # åˆ›å»º ClientSession å¯¹è±¡ï¼Œç”¨äºä¸æœåŠ¡å™¨é€šä¿¡
                        session = await self.exit_stack.enter_async_context(ClientSession(stdio, write))
                        # åˆå§‹åŒ–ä¼šè¯
                        await session.initialize()
                        self.session_list.append(session)
                        # åˆ—å‡ºå¯ç”¨å·¥å…·
                        response = await session.list_tools()
                        tools = response.tools
                        # æ„å»ºå·¥å…·åˆ—è¡¨å­—ç¬¦ä¸²
                        tools_list = []
                        for tool in tools:
                            tool_str = f"- **{tool.name}**:\n  ```\n  {tool.description}\n  ```"
                            tools_list.append(tool_str)
                        tools_text = "\n".join(tools_list)

                        self._console.print(Panel(
                            Markdown(f"""
# æœåŠ¡å™¨è¿æ¥æˆåŠŸ âœ…

## æœåŠ¡å™¨ä¿¡æ¯
- **åç§°**: {server_name}
- **çŠ¶æ€**: åœ¨çº¿ ğŸŸ¢

## å¯ç”¨å·¥å…· ğŸ› ï¸
{tools_text}
                            """),
                            title="[green]æœåŠ¡å™¨è¿æ¥çŠ¶æ€[/green]",
                            border_style="green"
                        ))
                        self.server_names.append(server_name)
                    except FileNotFoundError as e:
                        self._console.print(Panel(
                            Markdown(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: `{str(e)}`\n\nè¯·æ£€æŸ¥å‘½ä»¤å’Œå‚æ•°æ˜¯å¦æ­£ç¡®ã€‚"),
                            title=f"[bold red]æœåŠ¡å™¨ {server_name} å¯åŠ¨å¤±è´¥[/bold red]",
                            border_style="red",
                            expand=False
                        ))
                    except Exception as e:
                        self._console.print(Panel(
                            Markdown(f"âŒ è¿æ¥å¤±è´¥: `{str(e)}`"),
                            title=f"[bold red]æœåŠ¡å™¨ {server_name} å¯åŠ¨å¤±è´¥[/bold red]",
                            border_style="red",
                            expand=False
                        ))
            except Exception as e:
                self._console.print(Panel(
                    Markdown(f"âŒ MCPé…ç½®è§£æå¤±è´¥: `{str(e)}`\n\nå°†ä»¥æ— æœåŠ¡å™¨æ¨¡å¼è¿è¡Œã€‚"),
                    title="[bold red]é…ç½®é”™è¯¯[/bold red]",
                    border_style="red",
                    padding=(1, 2)
                ))

    async def process_query(self, query: str) -> str:
        """ä½¿ç”¨ OpenAI å’Œå¯ç”¨å·¥å…·å¤„ç†æŸ¥è¯¢"""
        
        # åˆ›å»ºæ¶ˆæ¯åˆ—è¡¨
        self.messages.append({"role": "user", "content": query})

        # å¤„ç†æ¶ˆæ¯
        tool_functions,response = self.get_response()
        # å¦‚æœæœ‰å·¥å…·è°ƒç”¨
        while tool_functions:
            self.messages.extend(response)
            #è¿­ä»£è°ƒç”¨å·¥å…·Â·
            for tool_function in tool_functions:
                tool_call_id = tool_function["id"]
                tool_name = tool_function["function"]["name"]
                logging.debug(f"å·¥å…·å‡½æ•°åŸå§‹æ•°æ®: {tool_function}")
                logging.debug(f"å·¥å…·å‡½æ•°å‚æ•°: {tool_function['function'].get('arguments', '')}")
                
                try:
                    tool_args = json.loads(tool_function["function"].get("arguments") or "{}")
                except json.JSONDecodeError as e:
                    print(f"JSONè§£æé”™è¯¯: {e}")
                    print(f"åŸå§‹å‚æ•°å­—ç¬¦ä¸²: '{tool_function['function'].get('arguments', '')}'")
                    tool_args = {}
                # å¤„ç†å‚æ•°æ˜¾ç¤º
                args_str = json.dumps(tool_args, ensure_ascii=False, indent=2)
                if len(args_str) > 1000:
                    args_str = args_str[:1000] + "..."

                self._console.print(Panel(
                    Markdown(f"**Tool Call**\n\nè°ƒç”¨å·¥å…·: {tool_name}\n\nå‚æ•°:\n\n```json\n{args_str}\n```"),
                    title="å·¥å…·è°ƒç”¨",
                    expand=False
                ))
                    
                # æ‰§è¡Œå·¥å…·è°ƒç”¨ç»“æœ
                tool_to_session = {tool["function"]["name"]: idx for idx, tools in enumerate(self.available_tools) for tool in tools}
                session_idx = tool_to_session.get(tool_name)
                if session_idx is not None:
                    session = self.session_list[session_idx]

                    if self.config.is_human_control:
                        # å¦‚æœæœ‰äººå·¥æ§åˆ¶ï¼Œåˆ™éœ€è¦äººå·¥ç¡®è®¤(å›è½¦é»˜è®¤ç¡®è®¤ï¼Œå¦åˆ™è¾“å…¥çš„æ˜¯æ‹’ç»ç†ç”±)
                        #ç”¨æˆ·ç¡®è®¤æ´é¢
                        is_confirm = self._console.input("æ˜¯å¦ç¡®è®¤è°ƒç”¨å·¥å…·? (å›è½¦ç¡®è®¤|æ‹’ç»ç†ç”±): ")
                    
                        if is_confirm == "":
                            result = await session.call_tool(tool_name, tool_args)
                        else:
                            
                            self._console.print(Panel(
                                Markdown(f"âŒ å·¥å…·è°ƒç”¨å·²å–æ¶ˆ\n\n**æ‹’ç»ç†ç”±**:\n> {is_confirm}"),
                                title="[red]å·¥å…·è°ƒç”¨å–æ¶ˆ[/red]",
                                border_style="red",
                                expand=False
                            ))
                            # é€’å½’è°ƒç”¨process_queryï¼Œé‡æ–°è·å–å·¥å…·è°ƒç”¨
                            query = f"æˆ‘æ‹’ç»ä½ ç”³è¯·çš„è°ƒç”¨å·¥å…·:{tool_name},å‚æ•°:{args_str[:200]} ï¼›ç†ç”±æ˜¯ï¼š{is_confirm}"
                            return await self.process_query(query)

                    else:
                        result = await session.call_tool(tool_name, tool_args)
                    logging.debug(f"å·¥å…·è°ƒç”¨å®Œæ•´ç»“æœ: {result}")
                else:
                    raise ValueError(f"Tool '{tool_name}' not found in available tools")
                self._console.print(Panel(
                    Markdown("**Result**\n\n" + "\n\n".join(content.text for content in result.content) + f"\n\nMeta: {str(result.meta)}\nIsError: {str(result.isError)}"),
                    title="å·¥å…·è°ƒç”¨ç»“æœ",
                    expand=False
                ))
                
                #å°†å·¥å…·è°ƒç”¨ç»“æœæ·»åŠ åˆ°æ¶ˆæ¯
                if not self.model.startswith("gpt"):
                    self.messages.append(
                    {
                        "role": "user", # gptå¯ä»¥ç”¨function
                        "tool_call_id": tool_call_id,
                        "name": tool_name,
                        "content": "Tool call result:\n"+str(
                            {
                                "type": "tool_result",
                                "tool_name": tool_name,
                                "tool_use_id": tool_call_id,
                                "result": [content.text for content in result.content],
                                "meta": str(result.meta),
                                "isError": str(result.isError)
                            })
                        }
                    )
                    
                    
                else:
                    self.messages.append({
                        "role": "user",
                        "tool_call_id": tool_call_id,
                        "name": tool_name,
                        "content": str(
                            {
                                "type": "tool_result",
                                "tool_use_id": tool_call_id,
                                "result": [content.text for content in result.content],
                                "meta": str(result.meta),
                                "isError": str(result.isError)
                            })
                        }
                    )
            # å°†è°ƒç”¨ç»“æœè¿”å›ç»™LLMï¼Œè·å–å›å¤
            tool_functions,response = self.get_response()
            
        # æœ€ç»ˆå“åº”    
        self.messages.extend(response)
        return self.messages
        
    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        await self.exit_stack.aclose() 

    def save_messages(self):
        """ä¿å­˜å¯¹è¯æ¶ˆæ¯åˆ°æ–‡ä»¶"""
        import time
        timestamp = int(time.time())
        filename = f"messages_{timestamp}.json"
        save_path = self.config.messages_path
        os.makedirs(save_path, exist_ok=True)
        with open(os.path.join(save_path, filename), "w", encoding="utf-8") as f:
            json.dump(self.messages, f, ensure_ascii=False, indent=4)

        return filename

    def manage_message_history(self):
        if self.config.max_messages_tokens > 0:
            # Trim message history if it exceeds the maximum length
            if len(self.messages) > self.config.max_messages:
                self.messages = self.messages[-self.config.max_messages:]
            
            # Ensure system prompt is at the beginning of the message history
            if self.config.system_prompt and (not self.messages or self.messages[0]["role"] != "system"):
                # æ£€æŸ¥ç³»ç»Ÿæç¤ºæ–‡ä»¶æ˜¯å¦å­˜åœ¨
                if os.path.exists(self.config.system_prompt):
                    try:
                        with open(self.config.system_prompt, "r", encoding="utf-8") as f:
                            system_prompt = f.read()
                        logging.debug(f"æˆåŠŸä»æ–‡ä»¶åŠ è½½ç³»ç»Ÿæç¤º: {self.config.system_prompt}")
                    except Exception as e:
                        logging.error(f"è¯»å–ç³»ç»Ÿæç¤ºæ–‡ä»¶å‡ºé”™: {e}")
                        system_prompt = "You are a helpful assistant."
                else:
                    # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤æç¤º
                    logging.warning(f"ç³»ç»Ÿæç¤ºæ–‡ä»¶ä¸å­˜åœ¨: {self.config.system_prompt}")
                    system_prompt = "You are a helpful assistant."
                self.messages.insert(0, {"role": "system", "content": system_prompt})
        else:
            # å¦‚æœè®¾ç½®å°äºç­‰äº0ï¼Œåˆ™ä¸é™åˆ¶æ¶ˆæ¯é•¿åº¦
            pass
    
    async def handle_command(self, command: str):
        if command == 'quit' or command == 'exit':
            return 'exit'
        elif command == 'fc':
            self.config.is_function_calling = not self.config.is_function_calling
            status = "å¯åŠ¨" if self.config.is_function_calling else "å–æ¶ˆ"
            # ä½¿ç”¨ä¸»æ§åˆ¶å°æ˜¾ç¤ºçŠ¶æ€æ›´æ–°
            self._console.print(Panel(
                Markdown(f"ğŸ› ï¸ å·¥å…·è°ƒç”¨åŠŸèƒ½å·²{status}"),
                title="[bold green]åŠŸèƒ½çŠ¶æ€æ›´æ–°[/bold green]",
                border_style="green"
            ))
            return 'fc'
        elif command == 'model':
            self.choose_model()
            return 'model'
        elif command == 'save':
            filename = self.save_messages()
            self._console.print(Panel(
                Markdown(f"ğŸ“ æ¶ˆæ¯å†å²å·²æˆåŠŸä¿å­˜\n\nğŸ“„ æ–‡ä»¶å: `{filename}`"),
                title="[bold blue]ä¿å­˜æˆåŠŸ[/bold blue]",
                border_style="cyan"
            ))
            return 'save'
        elif command == 'human':
            self.config.is_human_control = not self.config.is_human_control
            status = "å¯åŠ¨" if self.config.is_human_control else "å–æ¶ˆ"
            self._console.print(Panel(
                Markdown(f"ğŸ‘¤ äººç±»å¹²é¢„å·²{status}"),
                title="[bold green]åŠŸèƒ½çŠ¶æ€æ›´æ–°[/bold green]",
                border_style="green"
            ))
            return 'human'
        elif command.startswith("compact "):
            # è·å–ç”¨æˆ·æŒ‡å®šçš„å­—ç¬¦æ•°é™åˆ¶,é»˜è®¤200
            parts = command[8:].strip().split(" ")
            char_limit = 200  # é»˜è®¤å€¼
            if len(parts) > 0:
                try:
                    char_limit = int(parts[0])
                    if char_limit <= 0:
                        raise ValueError("å­—ç¬¦æ•°é™åˆ¶å¿…é¡»å¤§äº0")
                except ValueError as e:
                    self._console.print(Panel(
                        f"[red]é”™è¯¯: {str(e)}\nä½¿ç”¨é»˜è®¤å€¼200[/red]",
                        title="[bold red]å‚æ•°é”™è¯¯[/bold red]",
                        border_style="red"
                    ))
                    char_limit = 200
            
            # ç»Ÿè®¡å‹ç¼©ä¿¡æ¯
            compressed_count = 0
            total_saved = 0
            
            # å‹ç¼©æ¶ˆæ¯å†å²
            for message in self.messages:
                # æ£€æŸ¥æ˜¯å¦æœ‰contentå­—æ®µ
                if "content" not in message:
                    continue
                    
                content_length = len(message["content"])
                if content_length > char_limit:
                    original_length = len(message["content"])
                    message["content"] = message["content"][:char_limit] + "..."
                    compressed_count += 1
                    total_saved += original_length - len(message["content"])
            
            # æ˜¾ç¤ºè¯¦ç»†çš„å‹ç¼©ç»“æœ
            self._console.print(Panel(
                Markdown(f"""
# ğŸ“ æ¶ˆæ¯å†å²å‹ç¼©å®Œæˆ

- å­—ç¬¦æ•°é™åˆ¶: `{char_limit}`
- å‹ç¼©æ¶ˆæ¯æ•°: `{compressed_count}`
- èŠ‚çœå­—ç¬¦æ•°: `{total_saved}`
- å‹ç¼©æ¯”ä¾‹: `{compressed_count/len(self.messages)*100:.1f}%`
                """),
                title="[bold green]å‹ç¼©æˆåŠŸ[/bold green]",
                border_style="green"
            ))
            return 'compact'
        
        elif command.startswith('cost'):
            self.total_input_tokens,self.total_output_tokens,self.current_input_tokens,self.round_count=get_token_count(self.messages,self.model)
            self.total_input_tokens += sum(self.tool_tokens)
            if self.config.is_function_calling :
                # è®¡ç®—tool tokens
                encoding = tiktoken.encoding_for_model(self.model)
                name=[json.dumps(tool["function"]["name"]) for available_tool in self.available_tools for tool in available_tool]
                parameters=[json.dumps(tool["function"]["parameters"]) for available_tool in self.available_tools for tool in available_tool]
                description=[json.dumps(tool["function"]["description"]) for available_tool in self.available_tools for tool in available_tool]
                #æ¯ä¸ªå­—ç¬¦ä¸²çš„tokenæ•°ä¹‹å’Œä¸ºtool_tokens
                tool_tokens=sum([len(encoding.encode(n)) for n in name])+sum([len(encoding.encode(p)) for p in parameters])
                self.current_input_tokens += tool_tokens
            self._console.print(Panel(
                Markdown(f"""
# ğŸ“Š Token ä½¿ç”¨ç»Ÿè®¡æŠ¥å‘Š

- ğŸ”¹ è¾“å…¥æ¶ˆè€—æ€»è®¡: `{self.total_input_tokens}` Tokens
- ğŸ”¸ è¾“å‡ºæ¶ˆè€—æ€»è®¡: `{self.total_output_tokens+self.thinking_tokens}` Tokens
- ğŸ’­ æ€ç»´é“¾æ¶ˆè€—: `{self.thinking_tokens}` Tokens  
- â³ ä¸‹è½®é¢„ä¼°æ¶ˆè€—: `{self.current_input_tokens}` Tokens
- ğŸ” å½“å‰å¯¹è¯è½®æ¬¡: `{self.round_count}`
"""),
                title="[bold green]Tokenæ¶ˆè€—ç»Ÿè®¡[/bold green]",
                border_style="green"
            ))
            return 'cost'
        elif command.startswith('load '):
            file_path = command[5:].strip()
            try:
                file_path = os.path.expanduser(file_path)
                with open(file_path, "r", encoding="utf-8") as f:
                    loaded_messages = json.load(f)
                self.messages = loaded_messages
                self._console.print(Panel(
                    Markdown(f"ğŸ“‚ æ¶ˆæ¯å†å²å·²æˆåŠŸåŠ è½½\n\nğŸ“„ æ–‡ä»¶è·¯å¾„: `{file_path}`"),
                    title="[bold blue]åŠ è½½æˆåŠŸ[/bold blue]",
                    border_style="cyan"
                ))
        
            except FileNotFoundError:
                self._console.print(f"[bold red]é”™è¯¯ï¼šæ–‡ä»¶ '{file_path}' ä¸å­˜åœ¨[/bold red]")
            except json.JSONDecodeError:
                self._console.print(f"[bold red]é”™è¯¯ï¼šæ–‡ä»¶ '{file_path}' ä¸æ˜¯æœ‰æ•ˆçš„ JSON æ ¼å¼[/bold red]")
            except Exception as e:
                self._console.print(f"[bold red]åŠ è½½æ–‡ä»¶æ—¶å‡ºé”™ï¼š{str(e)}[/bold red]")
            return 'load'
        elif command.startswith('mcp '):
            try:
                parts = command[4:].strip().split(" ")
                if not parts or not parts[0]:
                    self._console.print(Panel(
                        Markdown("è¯·æä¾›MCPé…ç½®æ–‡ä»¶è·¯å¾„"),
                        title="[bold yellow]æç¤º[/bold yellow]",
                        border_style="yellow"
                    ))
                    return 'mcp_error'
                
                new_mcp_config_path = parts[0]
                if not os.path.exists(new_mcp_config_path):
                    self._console.print(Panel(
                        Markdown(f"é…ç½®æ–‡ä»¶ '{new_mcp_config_path}' ä¸å­˜åœ¨"),
                        title="[bold red]é”™è¯¯[/bold red]",
                        border_style="red"
                    ))
                    return 'mcp_error'
                
                self._console.print(Panel(
                    Markdown(f"æ­£åœ¨åˆ‡æ¢æ–°çš„MCPé…ç½®æ–‡ä»¶: {new_mcp_config_path}"),
                    title="[bold green]åˆ‡æ¢é…ç½®æ–‡ä»¶[/bold green]",
                    border_style="green"
                ))
                
                # æ¸…ç†æ—§çš„èµ„æº
                await self.cleanup()
                
                # é‡ç½®ä¼šè¯ç›¸å…³çš„å±æ€§
                self.server_names = []
                self.session_list = []
                self.available_tools = []
                self.exit_stack = AsyncExitStack()  # åˆ›å»ºæ–°çš„ exit_stack
                
                # æ›´æ–°é…ç½®
                self.config.mcp_server_config = new_mcp_config_path
                
                try:
                    # é‡æ–°è¿æ¥æœåŠ¡å™¨
                    await self.connect_to_server()
                    # è·å–æ–°çš„å·¥å…·åˆ—è¡¨
                    await self.get_tools()
                    
                    self._console.print(Panel(
                        Markdown("âœ… MCPé…ç½®åˆ‡æ¢å®Œæˆ"),
                        title="[bold green]æˆåŠŸ[/bold green]",
                        border_style="green"
                    ))
                    return 'mcp'
                except Exception as e:
                    self._console.print(Panel(
                        Markdown(f"æœåŠ¡å™¨è¿æ¥å¤±è´¥ï¼š\n\n```\n{str(e)}\n```"),
                        title="[bold red]é”™è¯¯[/bold red]",
                        border_style="red"
                    ))
                    return 'mcp_error'
                    
            except Exception as e:
                import traceback
                self._console.print(Panel(
                    Markdown(f"åˆ‡æ¢MCPé…ç½®æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯ï¼š\n\n```\n{str(e)}\n{traceback.format_exc()}\n```"),
                    title="[bold red]é”™è¯¯[/bold red]",
                    border_style="red"
                ))
                return 'mcp_error'
        

        
        elif command == "help":
            self._console.print(Panel(
                Markdown(self.get_help_text()),
                title="[bold cyan]ğŸ” å‘½ä»¤å¸®åŠ©ä¸­å¿ƒ[/bold cyan]",
                subtitle="[dim]è¾“å…¥ \\ å¯æŸ¥çœ‹ç®€æ´å‘½ä»¤åˆ—è¡¨[/dim]",
                border_style="cyan",
                padding=(1, 2)
            ))
            return 'help'
        elif command == "clear":
            # æ¸…ç©ºå†å²æ¶ˆæ¯
            self.messages = []
            self._console.print(Panel(
                "ğŸ§¹ å¯¹è¯å†å²å·²æ¸…ç©º",
                title="[bold green]æ“ä½œæˆåŠŸ[/bold green]",
                border_style="green"
            ))
            return 'clear'
        elif command == "debug":
            self.config.debug = not self.config.debug
            logging.basicConfig(level=logging.DEBUG if self.config.debug else logging.ERROR)
            self._console.print(Panel(
                f"è°ƒè¯•æ¨¡å¼{'å¼€å¯' if self.config.debug else 'å…³é—­'}",
                title="[bold blue]æ“ä½œæˆåŠŸ[/bold blue]",
                border_style="cyan"
            ))
            return 'debug'
        else:
            self._console.print(Panel(
                f"æœªçŸ¥å‘½ä»¤ï¼š\\{command}\nä½¿ç”¨ \\help æŸ¥çœ‹å¯ç”¨å‘½ä»¤",
                title="[bold yellow]æç¤º[/bold yellow]",
                border_style="yellow"
            ))
            return 'help'
            
    def get_multiline_input(self) -> str:
        """è·å–å¤šè¡Œè¾“å…¥
        
        æ”¯æŒä»¥ä¸‹ç‰¹æ€§ï¼š
        1. å¤šè¡Œè¾“å…¥ï¼ŒæŒ‰Enterç»§ç»­è¾“å…¥
        2. è¾“å…¥ \\q å•ç‹¬ä¸€è¡Œæ¥ç»“æŸè¾“å…¥
        3. è¾“å…¥ \\c å•ç‹¬ä¸€è¡Œæ¥æ¸…é™¤å½“å‰è¾“å…¥
        4. ç©ºè¡Œä¼šè¢«ä¿ç•™
        5. å‘½ä»¤è¡Œï¼ˆä»¥\\å¼€å¤´ï¼‰ä¼šç›´æ¥æ‰§è¡Œï¼Œä¸éœ€è¦\\q
        6. è¾“å…¥\\æ—¶ä¼šæ˜¾ç¤ºå‘½ä»¤æç¤º
        
        Returns:
            str: ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬
        """
        # åˆå§‹åŒ–readline
        try:
            # å®šä¹‰ä¸€ä¸ªç©ºçš„è¡¥å…¨å‡½æ•°
            def empty_completer(text, state):
                return None
                
            # å®Œå…¨ç¦ç”¨readlineçš„è¡¥å…¨åŠŸèƒ½
            readline.set_completer(empty_completer)
            readline.parse_and_bind('bind ^I rl_complete')  # ç¦ç”¨tabè¡¥å…¨
            readline.parse_and_bind('set disable-completion on')  # ç¦ç”¨æ‰€æœ‰è¡¥å…¨
            readline.parse_and_bind('set show-all-if-ambiguous off')  # ç¦ç”¨æ¨¡ç³ŠåŒ¹é…æ˜¾ç¤º
            readline.parse_and_bind('set show-all-if-unmodified off')  # ç¦ç”¨æœªä¿®æ”¹æ—¶çš„æ˜¾ç¤º
            readline.parse_and_bind('set completion-ignore-case off')  # ç¦ç”¨å¤§å°å†™å¿½ç•¥
            readline.parse_and_bind('set completion-query-items 0')  # ç¦ç”¨è¡¥å…¨é¡¹æŸ¥è¯¢
        except Exception as e:
            logging.warning(f"readlineåˆå§‹åŒ–å¤±è´¥: {e}")
            
        self._console.print(self.input_prompt, end="")
        lines = []
        while True:
            try:
                # ä½¿ç”¨readlineè·å–è¾“å…¥
                line = input().strip()
                # å¤„ç†å‘½ä»¤ï¼ˆä»¥\å¼€å¤´ï¼‰
                if line.startswith('\\'):
                    if len(lines) == 0:  # åªæœ‰åœ¨ç¬¬ä¸€è¡Œæ—¶æ‰å¤„ç†å‘½ä»¤
                        # å¦‚æœåªè¾“å…¥äº†\ï¼Œæ˜¾ç¤ºå‘½ä»¤æç¤º
                        if line == '\\':
                            self._show_command_suggestions()
                            self._console.print(self.input_prompt, end="")
                            continue
                        return line
                    elif line == r'\q':  # ä½¿ç”¨åŸå§‹å­—ç¬¦ä¸²æ¥åˆ¤æ–­
                        # æ˜¾ç¤ºè¾“å…¥å®Œæˆçš„æç¤º
                        self._console.print(Panel(
                            Group(
                                Spinner('dots', text='æ­£åœ¨å¤„ç†æ‚¨çš„è¾“å…¥...', style="cyan"),
                                Text("\nè¾“å…¥å·²å®Œæˆï¼Œå…± {} è¡Œ".format(len(lines)), style="dim")
                            ),
                            title="[bold green]âœ¨ è¾“å…¥å®Œæˆ[/bold green]",
                            border_style="green",
                            padding=(1, 2)
                        ))
                        break
                    elif line == r'\c':  # ä½¿ç”¨åŸå§‹å­—ç¬¦ä¸²æ¥åˆ¤æ–­
                        lines = []
                        self._console.print("[yellow]å·²æ¸…é™¤å½“å‰è¾“å…¥[/yellow]")
                        self._console.print(self.input_prompt, end="")
                        continue
                    else:  # å…¶ä»–\å¼€å¤´çš„å†…å®¹ä½œä¸ºæ™®é€šæ–‡æœ¬å¤„ç†
                        lines.append(line)
                else:
                    lines.append(line)
            except EOFError:
                break
            except KeyboardInterrupt:
                self._console.print("\n[yellow]å·²å–æ¶ˆå½“å‰è¾“å…¥[/yellow]")
                return ""
            
        # åˆå¹¶æ‰€æœ‰è¡Œ
        return '\n'.join(lines)

    def _show_command_suggestions(self):
        """æ˜¾ç¤ºå‘½ä»¤æç¤º"""
        # å®šä¹‰å‘½ä»¤åˆ†ç±»
        command_categories = {
            "åŸºç¡€å‘½ä»¤": [
                ("", "æ˜¾ç¤ºæ­¤å‘½ä»¤åˆ—è¡¨"),
                ("quit/exit", "é€€å‡ºç³»ç»Ÿ"),
                ("clear", "æ¸…ç©ºå½“å‰ä¼šè¯å†å²"),
                ("help", "æ˜¾ç¤ºè¯¦ç»†å¸®åŠ©ä¿¡æ¯")
            ],
            "æ¨¡å‹ä¸å·¥å…·": [
                ("model", "åˆ‡æ¢è¯­è¨€æ¨¡å‹"),
                ("fc", "å¼€å¯/å…³é—­å·¥å…·è°ƒç”¨"),
                ("human", "å¼€å¯/å…³é—­äººç±»å¹²é¢„"),
                ("mcp <è·¯å¾„>", "åˆ‡æ¢MCPé…ç½®æ–‡ä»¶")
            ],
            "ä¼šè¯ç®¡ç†": [
                ("save", "ä¿å­˜å½“å‰ä¼šè¯"),
                ("compact <å­—ç¬¦æ•°>", "å‹ç¼©æ¶ˆæ¯å†å²"),
                ("load <è·¯å¾„>", "åŠ è½½å†å²ä¼šè¯")
            ],
            "ç»Ÿè®¡ä¸è°ƒè¯•": [
                ("cost", "æ˜¾ç¤ºTokenç»Ÿè®¡"),
                ("debug", "åˆ‡æ¢è°ƒè¯•æ¨¡å¼")
            ]
        }
        
        # åˆ›å»ºè¡¨æ ¼
        table = Table(
            title="å¿«é€Ÿå‘½ä»¤å‚è€ƒ",
            box=ROUNDED,
            expand=False,
            padding=(0, 1),
            border_style="cyan",
            highlight=True
        )
        
        # æ·»åŠ åˆ—
        table.add_column("åˆ†ç±»", style="bold cyan", justify="left", no_wrap=True)
        table.add_column("å‘½ä»¤", style="green", justify="left")
        table.add_column("æè¿°", style="white", justify="left")
        
        # æ·»åŠ è¡Œ
        for category, commands in command_categories.items():
            for i, (cmd, desc) in enumerate(commands):
                if i == 0:
                    # ç¬¬ä¸€è¡Œæ˜¾ç¤ºåˆ†ç±»
                    table.add_row(
                        f"[bold]{category}[/bold]", 
                        f"[cyan]\\{cmd}[/cyan]" if cmd else "[cyan]\\[/cyan]", 
                        desc
                    )
                else:
                    # åç»­è¡Œä¸æ˜¾ç¤ºåˆ†ç±»
                    table.add_row(
                        "", 
                        f"[cyan]\\{cmd}[/cyan]", 
                        desc
                    )
        
        # æ˜¾ç¤ºè¡¨æ ¼
        self._console.print(Panel(
            Group(
                table,
                Text("\nğŸ’¡ æç¤º: è¾“å…¥ [cyan]\\help[/cyan] è·å–æ›´è¯¦ç»†çš„å‘½ä»¤è¯´æ˜", style="dim")
            ),
            title="[bold cyan]ğŸ” å¯ç”¨å‘½ä»¤[/bold cyan]",
            border_style="cyan",
            padding=(1, 2)
        ))

    async def chat_loop(self):
        """è¿è¡Œäº¤äº’å¼èŠå¤©å¾ªç¯"""
        server_message = "## ğŸ–¥ï¸ å·²è¿æ¥æœåŠ¡å™¨\n" + "\n".join(f"- `{path}`" for path in self.server_names)
        
        # ASCIIè‰ºæœ¯æ ‡å¿—
        ascii_logo = """
   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘        â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘        â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
  â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
  â•šâ•â•  â•šâ•â• â•šâ•â•â•â•â•â•   â•šâ•â•   â•šâ•â• â•šâ•â•â•â•â•â• â•šâ•â•  â•šâ•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•
"""
        
        # åˆ›å»ºæ¬¢è¿ç•Œé¢ç»„ä»¶
        welcome_components = [
            Text(ascii_logo, style="cyan bold"),
            Text(""),
            Text("         æ™ºèƒ½AIåŠ©æ‰‹ | å¼ºå¤§å·¥å…·é“¾ | å¤šæ¨¡å‹æ”¯æŒ", style="green bold"),
            Text(""),
            Text("âœ¨ è¾“å…¥ \\ æŸ¥çœ‹æ‰€æœ‰å¯ç”¨å‘½ä»¤", style="yellow"),
            Text("ğŸ“š è¾“å…¥ \\help è·å–è¯¦ç»†å¸®åŠ©", style="yellow"),
            Text("ğŸ”„ è¾“å…¥ \\model åˆ‡æ¢AIæ¨¡å‹", style="yellow")
        ]
        
        # å¦‚æœæœ‰æœåŠ¡å™¨è¿æ¥ï¼Œæ·»åŠ æœåŠ¡å™¨ä¿¡æ¯
        if self.server_names:
            welcome_components.extend([
                Text(""),
                Markdown(server_message)
            ])
        
        # æ˜¾ç¤ºæ¬¢è¿ç•Œé¢
        self._console.print(Panel(
            Group(*welcome_components),
            title="[bold cyan]ActionAI æ™ºèƒ½åŠ©æ‰‹[/bold cyan]",
            subtitle="[dim]è¾“å…¥é—®é¢˜å¼€å§‹å¯¹è¯ï¼Œè¾“å…¥ \\ æŸ¥çœ‹å‘½ä»¤[/dim]",
            border_style="cyan",
            padding=(1, 2)
        ))
        
        while True:
            try:
                
                # å…ˆè·å–ç”¨æˆ·è¾“å…¥
                query = self.get_multiline_input().strip()
                
                # å¦‚æœç›´æ¥å›è½¦ï¼Œåˆ™è·³è¿‡
                if not query:
                    continue
                    
                # å¤„ç†æ‰€æœ‰\å¼€å¤´çš„å‘½ä»¤
                if query.startswith('\\'):
                    command = query[1:].strip().lower()
                    result = await self.handle_command(command)
                    if result == 'exit':
                        break
                    # å…¶ä½™å‘½ä»¤è·³è¿‡æ‰§è¡Œé—®ç­”
                    continue
                
                # åœ¨å¤„ç†å®é™…å¯¹è¯ä¹‹å‰æ£€æŸ¥tokené™åˆ¶
                self.manage_message_history()
                self.total_input_tokens, self.total_output_tokens, self.current_input_tokens, self.round_count = get_token_count(self.messages, self.model)
                # å¦‚æœæ­¤æ¬¡å¯¹è¯æ˜¯å·¥å…·è°ƒç”¨ï¼Œåˆ™éœ€è¦è®¡ç®—tool tokens
                if self.config.is_function_calling:
                    encoding = tiktoken.encoding_for_model(self.model)
                    name=[json.dumps(tool["function"]["name"]) for available_tool in self.available_tools for tool in available_tool]
                    parameters=[json.dumps(tool["function"]["parameters"]) for available_tool in self.available_tools for tool in available_tool]
                    description=[json.dumps(tool["function"]["description"]) for available_tool in self.available_tools for tool in available_tool]
                    #æ¯ä¸ªå­—ç¬¦ä¸²çš„tokenæ•°ä¹‹å’Œä¸ºtool_tokens
                    tool_tokens=sum([len(encoding.encode(n)) for n in name])+sum([len(encoding.encode(p)) for p in parameters])
                    self.current_input_tokens += tool_tokens
                    
                #è®¡ç®—queryçš„tokenæ•°
                encoding = tiktoken.encoding_for_model(self.model)
                query_tokens = encoding.encode(query)
                self.current_input_tokens += len(query_tokens)
                
                if self.current_input_tokens > self.config.max_messages_tokens:
                    self._console.print(Panel(
                        Markdown("""
                        # âš ï¸ å¯¹è¯é•¿åº¦è¶…å‡ºé™åˆ¶
                        
                        å½“å‰Tokenæ•°ï¼š{current}
                        æœ€å¤§é™åˆ¶ï¼š{max}
                        
                        å»ºè®®æ“ä½œï¼š
                        1. ä½¿ç”¨ `\\clear` æ¸…ç©ºå¯¹è¯å†å²
                        2. ä½¿ç”¨ `\\compact` å‹ç¼©å†å²æ¶ˆæ¯
                        3. ä½¿ç”¨ `\\save` ä¿å­˜å½“å‰å¯¹è¯åå†æ¸…ç©º
                        """.format(
                            current=self.current_input_tokens,
                            max=self.config.max_messages_tokens
                        )),
                        title="[bold red]è­¦å‘Š[/bold red]",
                        border_style="red"
                    ))
                    continue
                    
                # å¤„ç†æ­£å¸¸çš„å¯¹è¯
                await self.process_query(query)
                    
            except Exception as e:
                self._console.print(f"[bold red]é”™è¯¯: {str(e)}[/bold red]")
    
    def get_help_text(self):
        """è¿”å›å¸®åŠ©ä¿¡æ¯æ–‡æœ¬"""
        return """
# ğŸš€ ActionAI å‘½ä»¤æŒ‡å—

## ğŸ“‹ åŸºç¡€å‘½ä»¤

| å‘½ä»¤ | æè¿° | ç¤ºä¾‹ |
|------|------|------|
| `\\` | æ˜¾ç¤ºç®€æ´å‘½ä»¤åˆ—è¡¨ | `\\` |
| `\\quit` æˆ– `\\exit` | é€€å‡ºç³»ç»Ÿ | `\\quit` |
| `\\clear` | æ¸…ç©ºå½“å‰ä¼šè¯å†å² | `\\clear` |
| `\\help` | æ˜¾ç¤ºæ­¤å¸®åŠ©ä¿¡æ¯ | `\\help` |

## ğŸ¤– æ¨¡å‹ä¸å·¥å…·æ§åˆ¶

| å‘½ä»¤ | æè¿° | ç¤ºä¾‹ |
|------|------|------|
| `\\model` | åˆ‡æ¢è¯­è¨€æ¨¡å‹ | `\\model` |
| `\\fc` | å¼€å¯/å…³é—­å·¥å…·è°ƒç”¨ | `\\fc` |
| `\\human` | å¼€å¯/å…³é—­äººç±»å¹²é¢„æ¨¡å¼ | `\\human` |
| `\\mcp <é…ç½®æ–‡ä»¶è·¯å¾„>` | åˆ‡æ¢MCPé…ç½®æ–‡ä»¶ | `\\mcp ./config.json` |

## ğŸ’¾ ä¼šè¯ç®¡ç†

| å‘½ä»¤ | æè¿° | ç¤ºä¾‹ |
|------|------|------|
| `\\save` | ä¿å­˜å½“å‰ä¼šè¯åˆ°æ–‡ä»¶ | `\\save` |
| `\\load <æ–‡ä»¶è·¯å¾„>` | åŠ è½½å†å²ä¼šè¯ | `\\load ./messages_1234567890.json` |
| `\\compact <å­—ç¬¦æ•°>` | å‹ç¼©æ¶ˆæ¯å†å² | `\\compact 200` |

## ğŸ“Š ç»Ÿè®¡ä¸è°ƒè¯•

| å‘½ä»¤ | æè¿° | ç¤ºä¾‹ |
|------|------|------|
| `\\cost` | æ˜¾ç¤ºTokenä½¿ç”¨ç»Ÿè®¡ | `\\cost` |
| `\\debug` | åˆ‡æ¢è°ƒè¯•æ¨¡å¼ | `\\debug` |

## ğŸ’¡ è¾“å…¥æŠ€å·§

- å¤šè¡Œè¾“å…¥ï¼šæŒ‰Enterç»§ç»­è¾“å…¥
- ç»“æŸè¾“å…¥ï¼šè¾“å…¥ `\\q` å•ç‹¬ä¸€è¡Œ
- æ¸…é™¤è¾“å…¥ï¼šè¾“å…¥ `\\c` å•ç‹¬ä¸€è¡Œ
"""

async def main():
    """
    ä¸»å‡½æ•°
    """
    # åˆ›å»ºå®¢æˆ·ç«¯å®ä¾‹
    config = load_config_from_env(base_path)
    client = LLM_Client(config)
    #é€‰æ‹©æ¨¡å‹
    client.choose_model()
    try:
        #è¿æ¥æœåŠ¡å™¨
        await client.connect_to_server()
        # åˆ—å‡ºå¯ç”¨å·¥å…·
        await client.get_tools()
        # è¿è¡ŒèŠå¤©å¾ªç¯
        await client.chat_loop()
    finally:
        # ç¡®ä¿åœ¨ä»»ä½•æƒ…å†µä¸‹éƒ½æ¸…ç†èµ„æº
        await client.cleanup()

if __name__ == "__main__":
    asyncio.run(main())
